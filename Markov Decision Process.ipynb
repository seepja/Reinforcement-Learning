{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 9: Markov Decision Process\n",
    "\n",
    "Seepja Payasi\n",
    "\n",
    "\n",
    "\n",
    "A Markov Decision Process (MDP) model contains: \n",
    "\n",
    "A set of possible world states S.\n",
    "\n",
    "A set of Models.\n",
    "\n",
    "A set of possible actions A.\n",
    "\n",
    "A real-valued reward function R(s,a).\n",
    "\n",
    "A policy the solution of Markov Decision Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 6.\n",
      "\n",
      "\n",
      "\n",
      "[[0.0688909  0.06141457 0.07440976 0.05580732]\n",
      " [0.09185454 0.         0.11220821 0.        ]\n",
      " [0.14543635 0.24749695 0.29961759 0.        ]\n",
      " [0.         0.3799359  0.63902015 0.        ]]\n",
      "\n",
      "\n",
      "[[0. 3. 0. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n",
      "Success Rate: 79.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def main():\n",
    "    # Starting new environment\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "\n",
    "    # Run policy Iteration\n",
    "    optimal_policy, optimal_value_function = policy_iteration(env, gamma=0.9)\n",
    "\n",
    "    # Print the optimal policy and optimal value function\n",
    "    print_output(optimal_policy, optimal_value_function)\n",
    "\n",
    "    # Test the optimal policy\n",
    "    run_policy(optimal_policy)\n",
    "\n",
    "def policy_iteration(env,gamma = 1.0):\n",
    "    # Initialize a random policy. Hint - Check numpy function np.zeros.\n",
    "    random_policy = np.zeros(env.observation_space.n)\n",
    "    no_of_iterations = 2000\n",
    "\n",
    "    for i in range(no_of_iterations):\n",
    "        new_value_function = policy_evaluation(env, random_policy, gamma)\n",
    "        new_policy = policy_improvement(env, new_value_function, gamma)\n",
    "        if (np.all(random_policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        random_policy = new_policy\n",
    "    return new_policy, new_value_function\n",
    "\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1.0):\n",
    "    # Initialize a value function with all zeros. Hint - Check numpy function np.zeros.\n",
    "    value_table = np.zeros(env.nS)\n",
    "\n",
    "    # Delta threshold as termination condition\n",
    "    threshold = 1e-10\n",
    "\n",
    "    while True:\n",
    "        # Make a copy of current value function to calculate delta change. Hint - numpy function for copy\n",
    "        updated_value_table = np.copy(value_table)\n",
    "\n",
    "        # Iterate over all the states\n",
    "        for state in range(env.nS):\n",
    "            # Select action for the state from policy being evaluated\n",
    "            action = policy[state]\n",
    "\n",
    "            # Calculate value of this state. Refer to slide 15 V(s) update. Transition probabilities can be accessed by env.P\n",
    "            value_table[state] = sum(\n",
    "                [trans_prob * (reward_prob + gamma * updated_value_table[next_state])\n",
    "                 for trans_prob, next_state, reward_prob, _ in env.P[state][action]])\n",
    "\n",
    "            s = 0\n",
    "            for trans_prob, next_state, reward_prob, _ in env.P[state][action]:\n",
    "                s += trans_prob * (reward_prob + gamma * updated_value_table[next_state])\n",
    "            value_table[state] = s\n",
    "        if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold):\n",
    "            break\n",
    "    return value_table\n",
    "\n",
    "\n",
    "def policy_improvement(env, value_table, gamma=1.0):\n",
    "    # Initialize array for policy with all zeros. Hint - Check numpy function np.zeros.\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "    # Iterate over all the states.\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q_table = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_sr in env.P[state][action]:\n",
    "                trans_prob, next_state, reward_prob, _ = next_sr\n",
    "\n",
    "                # Implement (transition probability + gamma * value function of a next state)\n",
    "                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))\n",
    "\n",
    "        # find out for which index (action) Q_table has max  value and assign it to state in policy array. Hint numpy argmax function.\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def print_output(policy=None, value_fn = None):\n",
    "    print(\"\\n\\n\")\n",
    "    print(np.reshape(value_fn, (-1, 4)))\n",
    "    print(\"\\n\")\n",
    "    print(np.reshape(policy, (-1, 4)))\n",
    "\n",
    "def run_policy(optimal_policy):\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "\n",
    "    # Implement this using lab 1 code to test optimal policy\n",
    "\n",
    "    # From lab 1 code, instead of random action, now choose action from optimal policy\n",
    "\n",
    "    # Calculate successful episodes %\n",
    "    num_episodes = 100\n",
    "    success = 0\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        new_state = env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            steps += 1\n",
    "\n",
    "            #  Select optimal action\n",
    "            action = optimal_policy[new_state]\n",
    "            # action = env.action_space.sample()\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # env.render()\n",
    "            if done:\n",
    "                if reward:\n",
    "                    success += 1\n",
    "                break\n",
    "\n",
    "    #  Close the environment\n",
    "    env.close()\n",
    "    env.env.close()\n",
    "\n",
    "    #  Calculate average number of steps we survived for all the episodes\n",
    "    print(\"Success Rate: {}%\".format((success / num_episodes) * 100))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
